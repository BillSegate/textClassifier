Tensor networks or tensor network states are a class of variational wave functions used in the study of many-body quantum systems.  Tensor networks extend one-dimensional matrix product states to higher dimensions while preserving some of their useful mathematical properties.
The wave function is encoded as a tensor contraction of a network of individual tensors.  The structure of the individual tensors can impose global symmetries on the wave function (such as antisymmetry under exchange of fermions) or restrict the wave function to specific quantum numbers, like total charge, angular momentum, or spin.  It is also possible to derive strict bounds on quantities like entanglement and correlation length using the mathematical structure of the tensor network.  This has made tensor networks useful in theoretical studies of quantum information in many-body systems.  They have also proved useful in variational studies of ground states, excited states, and dynamics of strongly correlated many-body systems.

Diagrammatic notation
In general, a tensor network diagram (Penrose diagram) can be viewed as a graph where nodes (or vertices) represent individual tensors, while edges represent summation over an index. Free indices are depicted as edges (or legs) attached to a single vertex only. Sometimes, there is also additional meaning to a node's shape. For instance, one can use trapezoids for unitary matrices or tensors with similar behaviour. This way, flipped trapezoids would be interpreted as complex conjugates to them.

Connection to machine learning
Tensor networks have been adapted for supervised learning, taking advantage of similar mathematical structure in variational studies in quantum mechanics and large-scale machine learning.  This crossover has spurred collaboration between researchers in artificial intelligence and quantum information science.  In June 2019, Google, the Perimeter Institute for Theoretical Physics, and X (company), released TensorNetwork, an open-source library for efficient tensor calculations.The main interest in tensor networks and their study from the perspective of machine learning is to reduce the number of trainable parameters (in a layer) by approximating a high-order tensor with a network of lower-order ones. Using the so-called tensor train technique (TT), one can reduce an N-order tensor (containing exponentially many trainable parameters) to a chain of N tensors of order 2 or 3, which gives us a polynomial number of parameters.

See also
Tensor
Tensor diagrams
Tensor contraction
Tensor Processing Unit (TPU)
Tensor rank decomposition
Einstein Notation


== References ==